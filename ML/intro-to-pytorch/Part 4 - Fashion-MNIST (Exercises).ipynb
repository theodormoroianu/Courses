{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 0/26421880 [00:00<?, ?it/s]Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n 99%|█████████▉| 26271744/26421880 [00:09<00:00, 3004988.57it/s]Extracting /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw\n\n0it [00:00, ?it/s]\u001b[A\n32768it [00:00, 194563.33it/s]\n\n0it [00:00, ?it/s]\u001b[ADownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\nExtracting /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A\n  2%|▏         | 81920/4422102 [00:00<00:06, 669372.07it/s]\u001b[A\n  6%|▋         | 278528/4422102 [00:00<00:04, 831978.39it/s]\u001b[A\n 12%|█▏        | 516096/4422102 [00:00<00:03, 1018785.64it/s]\u001b[A\n 18%|█▊        | 778240/4422102 [00:00<00:02, 1239339.87it/s]\u001b[A\n 22%|██▏       | 974848/4422102 [00:00<00:02, 1384691.89it/s]\u001b[A\n 27%|██▋       | 1187840/4422102 [00:00<00:02, 1528092.83it/s]\u001b[A\n 33%|███▎      | 1441792/4422102 [00:00<00:01, 1714028.04it/s]\u001b[A\n 37%|███▋      | 1646592/4422102 [00:00<00:01, 1799240.27it/s]\u001b[A\n 44%|████▍     | 1941504/4422102 [00:01<00:01, 2019424.26it/s]\u001b[A\n 51%|█████     | 2244608/4422102 [00:01<00:00, 2231010.17it/s]\u001b[A\n 58%|█████▊    | 2572288/4422102 [00:01<00:00, 2458016.02it/s]\u001b[A\n 66%|██████▌   | 2924544/4422102 [00:01<00:00, 2675137.02it/s]\u001b[A\n 74%|███████▎  | 3260416/4422102 [00:01<00:00, 2835565.14it/s]\u001b[A\n 81%|████████  | 3563520/4422102 [00:01<00:00, 1932540.66it/s]\u001b[A\n4423680it [00:01, 2259796.03it/s]\n\n8192it [00:00, 86371.58it/s]\nExtracting /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/theodor/.pytorch/F_MNIST_data/FashionMNIST/raw\nProcessing...\nDone!\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7fecb020bf50>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 231.84 \nL 231.84 231.84 \nL 231.84 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 7.2 224.64 \nL 224.64 224.64 \nL 224.64 7.2 \nL 7.2 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p402301f2c0)\">\n    <image height=\"218\" id=\"image1bd32719a4\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\"/>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\"/>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\"/>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\"/>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\"/>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\"/>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\"/>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\"/>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\"/>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\"/>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p402301f2c0\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 5, (3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 1, (3, 3), padding=(1, 1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(784, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.linear(self.conv(image.view(-1, 1, 28, 28)).view(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the network, define the criterion and optimizer\n",
    "model = Network()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1792 x 28], m2: [784 x 300] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-86c7cb3aa77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-07b6047c3f64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1792 x 28], m2: [784 x 300] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "# TODO: Train the network here\n",
    "\n",
    "epoch = 3\n",
    "\n",
    "for _ in range(epoch):\n",
    "    running_loss = 0.\n",
    "    for images, labels in testloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(images), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss)\n",
    "    print(\"Loss is %f\" % (running_loss / len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "ps = \n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}